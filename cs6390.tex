\documentclass[10pt,landscape]{article}
\usepackage{multicol}
\usepackage{amsfonts}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{hyperref}
\usepackage{stmaryrd}
\usepackage{amsmath}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\DeclareMathOperator*{\argmax}{arg\,max}


% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
	{ \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
	{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	}

% Turn off header and footer
\pagestyle{empty}
 

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}


% -----------------------------------------------------------------------

\begin{document}

\raggedright
\footnotesize
\begin{multicols}{3}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
     \Large{\textbf{IFT6390 Cheat Sheet}} \\
     Thomas George 2015
\end{center}

\section{Prerequisites}

\textbf{Isotropic gaussian distribution}
$$\mathcal{N}_{X, \sigma^2}(X_i) = \frac{1}{(2 \pi)^\frac{d}{2} \sigma^d} \exp \Big(-\frac{1}{2} \frac{d(X_i, X)^2}{\sigma^2} \Big)$$

\textbf{Distance properties}: $d(a,b)>0$, $d(a, a)=0$ and $d(a, b) + d(b, c) \geq d(a,c)$

\textbf{Kullbackâ€“Leibler divergence} (=relative entropy): $D_{\mathrm{KL}}(P\|Q) = \sum_i P(i) \, \log\frac{P(i)}{Q(i)}$

\section{Problem types}

$x$ is a feature vector, $y$ is the variable to predict

\newlength{\MyLen}
\settowidth{\MyLen}{\texttt{letterpaper}/\texttt{a4paper} \ }
\begin{tabular}{@{}p{\the\MyLen}%
                @{}p{\linewidth-\the\MyLen}@{}}
Classification &
Predict the class of $x$\newline
Classification error: $I_{f(x) \neq y}$\\
Regression &
Predict the value of $y$\newline
Quadratic error: $\sum(f(x) - y)^2$\\

Density estimation &
Probability of a new observation to be from the same probability density that of the training set\newline
Negative log-likelihood: $-\log(f(x))$
\end{tabular}

\subsection{Classification}

Binary classification: The discriminative fonction is compared to a threshold (ex $0$ or $0.5$) to get a decision function\\
Multiclass : Decision = $\argmax(g(x))$\\
one-hot encoding: bits vector where all elements are zeros except for the one that corresponds to the class

\section{k-NN}
\begin{tabular}{@{}p{\the\MyLen}%
                @{}p{\linewidth-\the\MyLen}@{}}
Binary classification & sign of the mean of the target value of the k nearest neighbors\\
Regression & mean of the target value of the k nearest neighbors\\


Multiclass classification & $\argmax(f(onehot))$
\end{tabular}

\section{Decision trees}
All trees can be represented as binary trees.

\textbf{Class frequencies} $\mu_j = \frac{\#\{class = C_j\}}{n}$

\textbf{Construction rule for nodes} : greedily maximize impurity decrease:
$$\Delta i(N) = i(N) - \mu^{(g)} i(N^{(g)}) - (1 - \mu^{(g)}) i (N^{(d)})$$

\begin{tabular}{@{}p{\the\MyLen}%
                @{}p{\linewidth-\the\MyLen}@{}}
Entropy & $i(N) = -\sum_j \mu_j \log(\mu_j)$\\
Variance & $i(N) = \mu_1 \mu_2$ (binary)\\
Gini & $i(N) = \sum_{i \neq j} \mu_i \mu_j$\\
Bad classification & $i(N)=1 - \max_j(\mu_j)$


\end{tabular}

For efficiency purpose, first sort all training samples given all input dimensions. Construction in $O(dn \log(n))$

\textbf{Oblique decision tree} : Use a linear discriminant function instead.

\section{Ensemble methods}

\subsection{Bagging}

\textbf{Bootstrap} : Sample $T$ times $m$ elements from the training set $\mathcal{D}_t$.

Construct a predictor on each set $h_t = A(\mathcal{D}_t)$. The resulting predictor consists in averaging all predictors:
$$f(x) = \frac{1}{T} \sum_{t=1}^T h_t(x)$$

\subsection{Boosting}

\begin{enumerate}
\item Learn a predictor $h_n$
\item Learn a predictor $h_{n+1}$ focusing on training examples misclassified by $h_n$
\item Combine $h_{n+1}$ with all $h_i, i<n$
\end{enumerate}

$$f(x) = \sum_{t=1}^T \alpha_t h_t(x) \, , \alpha_t > 0$$

\textbf{Weighted classification error} $A(\mathcal{D}, D) = \arg \min_{h \in \mathcal{H}} \sum_{i=1}^m D(i) I_{\{h(x_i) \neq y_i\}}$

\end{multicols}
\end{document}
