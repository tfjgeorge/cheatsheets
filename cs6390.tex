\documentclass[10pt,landscape]{article}
\usepackage{multicol}
\usepackage{amsfonts}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{hyperref}
\usepackage{stmaryrd}
\usepackage{amsmath}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\DeclareMathOperator*{\argmax}{arg\,max}


% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
	{ \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
	{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	}

% Turn off header and footer
\pagestyle{empty}
 

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}


% -----------------------------------------------------------------------

\begin{document}

\raggedright
\footnotesize
\begin{multicols}{3}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
     \Large{\textbf{IFT6390 Cheat Sheet}} \\
     Thomas George 2015
\end{center}

\section{Prerequisites}

\textbf{Isotropic gaussian distribution}
$$\mathcal{N}_{\mu, \sigma^2}(x) = \frac{1}{(2 \pi)^\frac{d}{2} \sigma^d} \exp \Big(-\frac{1}{2} \frac{d(x, \mu)^2}{\sigma^2} \Big)$$

\textbf{General $d$ dimensional gaussian distribution}
$$\mathcal{N}_{\mu, \Sigma}(x) = \frac{1}{(2 \pi)^\frac{d}{2} \sqrt{|\Sigma|}} \exp \Big(-\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x- \mu) \Big)$$

\textbf{Distance properties}: $d(a,b)>0$, $d(a, a)=0$ and $d(a, b) + d(b, c) \geq d(a,c)$

\textbf{Kullbackâ€“Leibler divergence} (=relative entropy): $D_{\mathrm{KL}}(P\|Q) = \sum_i P(i) \, \log\frac{P(i)}{Q(i)}$

\textbf{Mutual information}: $I(X_1, X_2) = D_{KL}(P(X_1, X_2) || P(X_1) P(X_2))$

\subsection{Model selection}

\textbf{IID dataset}: All random variables are independent and follow the same distribution.

\textbf{Bias/variance trade-off}: The model does not contain a perfect function/the data does not cover all cases

\textbf{Small training dataset}: Cross validation

\subsection{Hyperparameters}

\begin{itemize}
\item Control the size of the function family
\item Guide the model toward a certain preference
\item Choose a function family amongst several families
\item Control optimization
\end{itemize}


\subsection{Cost functions}
\textbf{Cross-entropy}:
$$L(y,t) = -(t \ln(y) + (1-t) \ln(1-y))$$

\textbf{Quadratic error}: $$L(y, t) = \lVert y-t \rVert^2 = \sum_{k=1}^m(y_k - t_k)^2$$

\section{Problem types}

$x$ is a feature vector, $y$ is the variable to predict

\newlength{\MyLen}
\settowidth{\MyLen}{\texttt{letterpaper}/\texttt{a4paper} \ }
\begin{tabular}{@{}p{\the\MyLen}%
                @{}p{\linewidth-\the\MyLen}@{}}
Classification &
Predict the class of $x$\newline
Classification error: $I_{f(x) \neq y}$\\
Regression &
Predict the value of $y$\newline
Quadratic error: $\sum(f(x) - y)^2$\\

Density estimation &
Probability of a new observation to be from the same probability density that of the training set\newline
Negative log-likelihood: $-\log(f(x))$
\end{tabular}

\subsection{Classification}

Binary classification: The discriminative fonction is compared to a threshold (ex $0$ or $0.5$) to get a decision function\\
Multiclass : Decision = $\argmax(g(x))$\\
one-hot encoding: bits vector where all elements are zeros except for the one that corresponds to the class

\textbf{A posteriori estimator}: $\hat p (x | C_i)$

\textbf{A priori estimator}: $\hat P (C_i)$

\section{Bayes}
$$P_{Y|X}(c|x) = \frac{p_{X|Y} (x|c) P_Y(c)}{p_X(x)}$$

\textbf{Naive bayes}: We suppose all $X_i$s are independent.

Naive bayes $\subset$ Bayes

\section{k-NN}
\begin{tabular}{@{}p{\the\MyLen}%
                @{}p{\linewidth-\the\MyLen}@{}}
Binary classification & sign of the mean of the target value of the k nearest neighbors\\
Regression & mean of the target value of the k nearest neighbors\\


Multiclass classification & $\argmax(f(onehot))$
\end{tabular}

\section{Decision trees}

\textbf{Pros}: Work with non-metric data, translation, scaling invariant, interpretability, efficient training and classification
\textbf{Cons}: Instability = adding a single point can drastically change the tree structure

\textbf{Class frequencies} $\mu_j = \frac{\#\{class = C_j\}}{n}$

\textbf{Construction rule for nodes} : greedily maximize impurity decrease:
$$\Delta i(N) = i(N) - \mu^{(g)} i(N^{(g)}) - (1 - \mu^{(g)}) i (N^{(d)})$$

\begin{tabular}{@{}p{\the\MyLen}%
                @{}p{\linewidth-\the\MyLen}@{}}
Entropy & $i(N) = -\sum_j \mu_j \log(\mu_j)$\\
Variance & $i(N) = \mu_1 \mu_2$ (binary)\\
Gini & $i(N) = \sum_{i \neq j} \mu_i \mu_j$\\
Bad classification & $i(N)=1 - \max_j(\mu_j)$


\end{tabular}

For efficiency purpose, first sort all training samples given all input dimensions. Construction in $O(dn \log(n))$

\textbf{Oblique decision tree} : Use a linear discriminant function instead.

\section{Linear regression}

$$f(x) = \langle w, x \rangle + b$$

Learn $b^*$ and $w^*$:

$$\Bigl(\negthinspace\begin{smallmatrix}b^* \\ w^* \end{smallmatrix}\Bigr) = (\check{X}^T\check{X} + \lambda \check{I})^{-1} \check{X}^T t$$
with $\check{X} =\begin{pmatrix}
1 & x_1^{(1)} & \cdots & x_d^{(1)}\\
\vdots &  \vdots &  \ddots &  \vdots\\
1 & x_1^{(n)} & \cdots & x_d^{(n)}\end{pmatrix}, t = \begin{pmatrix}t^{(1)} \\\vdots \\t^{(n)}\end{pmatrix}$

or learn using gradient descent.

\textbf{Regularization} Elastic net = lasso + ridge
$$\Omega(\theta)=\Omega(w, b) = \lambda_1 ||w_1||_1 + \lambda_2 ||w_2||_2^2$$

\subsection{Linear classifiers}
(when used in a binary classification problem)
\begin{itemize}
\item Nearest centroid
\item Bayes with gaussian densities
\item Linear regression with target $\{-1, 1\}$
\item Perceptron
\item Logistic regression
\item Linear SVM
\end{itemize}
\section{Support vector machines}

\textbf{Margin}: $\mathcal{H}_{w,b} = \{x | \langle w, x \rangle + b = 0\}$ where $x$ is the closest example.

Distance between a point and the hyperplan:
$$d(x, \mathcal{H}_{w,b}) = \Big| \frac{\langle w, x \rangle + b}{||w||} \Big| $$

\textbf{Find optimal hyperplan} that:
\begin{itemize}
\item Separate datas $(\langle w, x_i\rangle + b) y_i > 0$
\item Maximize the margin $w, b = \argmax_{w, b} \big\{ \min_i d(x_i, \mathcal{H}_{w,b}) \big\} = \argmax_{w, b} \big\{ \Big| \frac{\langle w, x \rangle + b}{\lVert w\rVert} \Big| y_i \big\}$
\end{itemize}

\textbf{Primal}: Minimize $\frac{1}{2} \lVert w \rVert ^2$ with constraints $y_i(w^Tx_i + b) \geq 1$
\textbf{Dual}: Maximize $\tilde L(\alpha) = \sum_{k=1}^n \alpha_k - \frac{1}{2} \sum_{i, j} \alpha_i \alpha_j y_i y_j x_i^T x_j$ with constraints $\alpha_i \geq 0$, et $\sum_{i=1}^n \alpha_i y_i = 0$

\section{Kernel trick}

\begin{itemize}
\item Usual dot product : $K(a, b) = \langle a, b \rangle$
\item Polynomial degree $k$: $K_k(a, b) = (1 + \langle a, b \rangle)^k$
\item RBF (=Gaussian): $K_{\sigma}(a, b) = e^{-\frac{1}{2} \frac{\lVert a-b\rVert^2}{\sigma^2}}$
\end{itemize}

\section{Ensemble methods}

Goal: improve stability (minimize variance) or the capacity (minimize bias)

\subsection{Bagging}
= \textbf{B}ootstrap + aver\textbf{aging}

\textbf{Bootstrap} : Sample $T$ times $m$ elements from the training set $\mathcal{D}_t$, construct a predictor on each set $h_t = A(\mathcal{D}_t)$.

\textbf{Average} : The resulting predictor consists in averaging all predictors:
$$f(x) = \frac{1}{T} \sum_{t=1}^T h_t(x)$$

\subsection{Boosting}

\begin{enumerate}
\item Learn a predictor $h_n$
\item Learn a predictor $h_{n+1}$ focusing on training examples misclassified by $h_n$
\item Combine $h_{n+1}$ with all $h_i, i<n$
\end{enumerate}

$$f(x) = \sum_{t=1}^T \alpha_t h_t(x) \, , \alpha_t > 0$$

\textbf{Weighted classification error} $A(\mathcal{D}, D) = \arg \min_{h \in \mathcal{H}} \sum_{i=1}^m D(i) I_{\{h(x_i) \neq y_i\}}$

\subsubsection{Adaboost}

\begin{enumerate}
\item Find $h_t = \arg \min_{h_j \in \mathcal{H}} \epsilon_j = \sum_{i=1}^m D_t(i) [y_i \neq h_j(x_i)] $.
\item If $\epsilon \geq 1/2$ then stop.
\item Set $\alpha_t = \frac{1}{2} \ln \big( \frac{1-\epsilon_t}{\epsilon_t} \big)$.
\item Update $D_{t+1}(i) = \frac{D_t(i) \exp(-\alpha_t y_i h_t(x_i))}{Z_t}$ where $Z_t$ is a normalization factor.
\end{enumerate}

This can be seen as a gradient descent optimization to find the best $h_t$ and the best $\alpha_t$.

\section{Clustering}

Partition a set of examples with no labels into $n$ regions.

\subsection{K-means}

Each point is assigned a cluster that is given by the closest of the $K$ mean points $\mu_i$ by repeating:
\begin{itemize}
\item For each point, assign a cluster by finding the closest $\mu_i$
\item Update $\mu_i$ with the mean of the points in the cluster $i$
\end{itemize}

\section{Principal component analysis}

Equivalently:

\begin{itemize}
\item Minimize the mean of the squared distances between $x_i$ and their projections
\item Maximize the variance of the projection
\end{itemize}

Algorithm:
\begin{enumerate}
\item Compute covariance matrix $S = \frac{1}{N} \sum_{n=1}^N (x_n - \bar x) (x_n - \bar x)^T$
\item Decompose into eigen vectors $S=U \Lambda U^T$
\item Conserve only desired $M$ eigen vectors $S \approx U_M \Lambda_M U_M^T$
\end{enumerate}

For a vector $x$ we get $z(x) = \Lambda_M^{-\frac{1}{2}} U_M^T (x - \bar x)$

Transformed points have a $cov = I$.

PCA is also \textbf{kernelizable}.

\subsection{Auto encoders}

An ANN that reconstructs its input with bottlenecks = hidden layers have less neurons than input.

For linear networks or networks with a single hidden layer, it is equivalent to PCA.

\section{Multilayer perceptron}

Cost function for a linear perceptron:
$$J(\theta) = \frac{1}{n} \sum_{i=1}^n I_{\{(w^Tx^{(i)} + b)t^{(i)} < 0\}} (-(w^Tx^{(i)} + b)t^{(i)})$$

\subsection{Logistic regression}
$$f_\theta(x) = f_{w, b}(x) = \text{sigmoid}(\langle w, x \rangle + b)$$

\subsubsection{Gradient descent}
$$\theta \leftarrow \theta - \eta \frac{\partial J}{\partial \theta}(\theta)$$

\textbf{Mini batch}: Apply gradient descent on smaller subsets

\textbf{Stochastic (SGD)}: mini batch with batch size $=1$

\section{Probabilistic graphical models}

\textbf{Usages}: For multivariate distributions, generate data, compute probability of a configuration, infer the most probable value of a subset of variables given observations, learn parameters of a distribution given samples.

\subsection{Definitions}
\begin{itemize}
\item $X$ marginally independent from $Y$ iff $
P(X, Y) = P(X)P(Y)$
\item $X$ conditionnally independant from $Y$ given $Z$ iff $P(X, Y | Z) = P(X|Z)P(Y|Z)$
\end{itemize}

In a graph $P(X_1, ..., X_d) = \prod_{i=1}^d P(X_i | Pa_i)$

\textbf{square} Categorical variable\\
\textbf{circle} Continuous variable

\subsection{Latent variables learning}

\textbf{Gradient descent} on the maximum likelihood; or

\textbf{Expectation maximization}: let $Z$ be the latent variables.

\begin{enumerate}
\item infer distribution $Z$ for each observed example $X$ given current parameters $\theta$ (expectation)
\item suppose $Z$ are observed and find parameters $\theta$ that maximize the likelihood
\end{enumerate}

\end{multicols}
\end{document}
